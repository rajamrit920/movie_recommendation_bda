{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seI460tSEi-h",
        "outputId": "f393e0ad-7437-4c96-a8a3-2fa8e53e5d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+-------+------+-------------------+\n",
            "|userId|movieId|rating|          timestamp|\n",
            "+------+-------+------+-------------------+\n",
            "|     1|      2|   3.5|2005-04-02 23:53:47|\n",
            "|     1|     29|   3.5|2005-04-02 23:31:16|\n",
            "|     1|     32|   3.5|2005-04-02 23:33:39|\n",
            "|     1|     47|   3.5|2005-04-02 23:32:07|\n",
            "|     1|     50|   3.5|2005-04-02 23:29:40|\n",
            "|     1|    112|   3.5|2004-09-10 03:09:00|\n",
            "|     1|    151|   4.0|2004-09-10 03:08:54|\n",
            "|     1|    223|   4.0|2005-04-02 23:46:13|\n",
            "|     1|    253|   4.0|2005-04-02 23:35:40|\n",
            "|     1|    260|   4.0|2005-04-02 23:33:46|\n",
            "|     1|    293|   4.0|2005-04-02 23:31:43|\n",
            "|     1|    296|   4.0|2005-04-02 23:32:47|\n",
            "|     1|    318|   4.0|2005-04-02 23:33:18|\n",
            "|     1|    337|   3.5|2004-09-10 03:08:29|\n",
            "|     1|    367|   3.5|2005-04-02 23:53:00|\n",
            "|     1|    541|   4.0|2005-04-02 23:30:03|\n",
            "|     1|    589|   3.5|2005-04-02 23:45:57|\n",
            "|     1|    593|   3.5|2005-04-02 23:31:01|\n",
            "|     1|    653|   3.0|2004-09-10 03:08:11|\n",
            "|     1|    919|   3.5|2004-09-10 03:07:01|\n",
            "+------+-------+------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#using als algorithm on user datset\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Step 1: Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"ALS Recommendation System\").getOrCreate()\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data = spark.read.csv(\"/content/rating.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Preview the data\n",
        "data.show()\n",
        "\n",
        "# Step 3: Preprocess data\n",
        "# Ensure columns are in the correct format\n",
        "data = data.select(\n",
        "    col(\"userId\").cast(\"integer\"),\n",
        "    col(\"movieId\").cast(\"integer\"),\n",
        "    col(\"rating\").cast(\"float\"),\n",
        "    col(\"timestamp\")\n",
        ")\n",
        "\n",
        "# Step 4: Split the dataset\n",
        "(training, test) = data.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Step 5: Configure ALS model\n",
        "als = ALS(\n",
        "    userCol=\"userId\",\n",
        "    itemCol=\"movieId\",\n",
        "    ratingCol=\"rating\",\n",
        "    nonnegative=True,  # Ensures no negative ratings\n",
        "    implicitPrefs=False,  # Set to False for explicit feedback\n",
        "    coldStartStrategy=\"drop\"  # Avoid NaN predictions\n",
        ")\n",
        "\n",
        "# Train the ALS model\n",
        "model = als.fit(training)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Use RMSE to evaluate predictions\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse}\")\n",
        "\n",
        "# Step 7: Generate recommendations\n",
        "# Top movie recommendations for each user\n",
        "user_recommendations = model.recommendForAllUsers(10)\n",
        "user_recommendations.show()\n",
        "\n",
        "# Top user recommendations for each movie\n",
        "movie_recommendations = model.recommendForAllItems(10)\n",
        "movie_recommendations.show()\n",
        "\n",
        "# Stop SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Hc-aQg4syluR",
        "outputId": "2b5fb92d-408e-434f-9cb0-8710ecd6a454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Preprocessing features...\n",
            "Creating feature vectors...\n",
            "\n",
            "Getting recommendations for movie ID 1...\n",
            "Input movie: Toy Story (1995)\n",
            "\n",
            "Top 5 recommendations:\n",
            "+-------+--------------------------+------------------------------------------------+----------+\n",
            "|movieId|title                     |genres                                          |similarity|\n",
            "+-------+--------------------------+------------------------------------------------+----------+\n",
            "|3114   |Toy Story 2 (1999)        |Adventure|Animation|Children|Comedy|Fantasy     |0.76821506|\n",
            "|78499  |Toy Story 3 (2010)        |Adventure|Animation|Children|Comedy|Fantasy|IMAX|0.7532216 |\n",
            "|106022 |Toy Story of Terror (2013)|Animation|Children|Comedy                       |0.7050196 |\n",
            "|4929   |Toy, The (1982)           |Comedy                                          |0.6424517 |\n",
            "|2274   |Lilian's Story (1995)     |Drama                                           |0.6130839 |\n",
            "+-------+--------------------------+------------------------------------------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
        "import numpy as np\n",
        "from pyspark.ml.feature import Normalizer\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MovieRecommender\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    # Load movies data\n",
        "    movies_df = spark.read.csv(\"/content/movie.csv\", header=True, inferSchema=True)\n",
        "    # Load ratings data for potential hybrid system extension\n",
        "    ratings_df = spark.read.csv(\"/content/rating.csv\", header=True, inferSchema=True)\n",
        "\n",
        "    # Clean movieId column - ensure it's numeric\n",
        "    movies_df = movies_df.withColumn(\"movieId\", col(\"movieId\").cast(\"integer\"))\n",
        "\n",
        "    return movies_df, ratings_df\n",
        "\n",
        "def preprocess_text_features(movies_df):\n",
        "    # Tokenization\n",
        "    tokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_tokens\",\n",
        "                             pattern=\"\\\\W\")\n",
        "\n",
        "    # Remove stop words\n",
        "    remover = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "    # TF (Term Frequency)\n",
        "    countVectorizer = CountVectorizer(inputCol=\"filtered_tokens\",\n",
        "                                    outputCol=\"title_tf\",\n",
        "                                    minDF=2.0)\n",
        "\n",
        "    # IDF (Inverse Document Frequency)\n",
        "    idf = IDF(inputCol=\"title_tf\", outputCol=\"title_tfidf\")\n",
        "\n",
        "    # Process genres\n",
        "    # Split genres string into array\n",
        "    split_genres = udf(lambda x: x.split(\"|\") if x else [],\n",
        "                      ArrayType(StringType()))\n",
        "    movies_df = movies_df.withColumn(\"genres_array\",\n",
        "                                    split_genres(col(\"genres\")))\n",
        "\n",
        "    # Create genre vectorizer\n",
        "    genreVectorizer = CountVectorizer(inputCol=\"genres_array\",\n",
        "                                    outputCol=\"genre_features\",\n",
        "                                    minDF=1.0)\n",
        "\n",
        "    # Create pipeline\n",
        "    pipeline = Pipeline(stages=[\n",
        "        tokenizer,\n",
        "        remover,\n",
        "        countVectorizer,\n",
        "        idf,\n",
        "        genreVectorizer\n",
        "    ])\n",
        "\n",
        "    # Fit and transform the data\n",
        "    model = pipeline.fit(movies_df)\n",
        "    processed_df = model.transform(movies_df)\n",
        "\n",
        "    return processed_df, model\n",
        "\n",
        "def create_feature_vector(processed_df):\n",
        "    # Combine title and genre features\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"title_tfidf\", \"genre_features\"],\n",
        "        outputCol=\"features\"\n",
        "    )\n",
        "\n",
        "    # Normalize the combined features\n",
        "    normalizer = Normalizer(inputCol=\"features\", outputCol=\"normalized_features\")\n",
        "\n",
        "    # Create and apply the pipeline\n",
        "    feature_pipeline = Pipeline(stages=[assembler, normalizer])\n",
        "    final_df = feature_pipeline.fit(processed_df).transform(processed_df)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def get_recommendations(movie_id, final_df, n=5):\n",
        "    # Get the feature vector for the input movie\n",
        "    input_movie = final_df.filter(col(\"movieId\") == movie_id).first()\n",
        "\n",
        "    if not input_movie:\n",
        "        raise ValueError(f\"Movie ID {movie_id} not found in the dataset\")\n",
        "\n",
        "    input_vector = input_movie.normalized_features\n",
        "\n",
        "    # Define cosine similarity function\n",
        "    def cosine_similarity(vec1, vec2):\n",
        "        return float(vec1.dot(vec2))\n",
        "\n",
        "    similarity_udf = udf(lambda x: cosine_similarity(input_vector, x),\n",
        "                        FloatType())\n",
        "\n",
        "    # Compute similarities and get top N recommendations\n",
        "    recommendations = final_df.filter(col(\"movieId\") != movie_id) \\\n",
        "        .withColumn(\"similarity\", similarity_udf(col(\"normalized_features\"))) \\\n",
        "        .orderBy(col(\"similarity\").desc()) \\\n",
        "        .limit(n)\n",
        "\n",
        "    return recommendations.select(\"movieId\", \"title\", \"genres\", \"similarity\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        print(\"Loading data...\")\n",
        "        movies_df, ratings_df = load_and_prepare_data()\n",
        "\n",
        "        # Preprocess features\n",
        "        print(\"Preprocessing features...\")\n",
        "        processed_df, pipeline_model = preprocess_text_features(movies_df)\n",
        "\n",
        "        # Create final feature vectors\n",
        "        print(\"Creating feature vectors...\")\n",
        "        final_df = create_feature_vector(processed_df)\n",
        "\n",
        "        # Cache the final dataframe for better performance\n",
        "        final_df.cache()\n",
        "\n",
        "        # Example usage\n",
        "        movie_id = 1  # Example movie ID\n",
        "        print(f\"\\nGetting recommendations for movie ID {movie_id}...\")\n",
        "\n",
        "        # Get the title of the input movie\n",
        "        input_movie_title = final_df.filter(col(\"movieId\") == movie_id).select(\"title\").first()\n",
        "        if input_movie_title:\n",
        "            print(f\"Input movie: {input_movie_title.title}\")\n",
        "\n",
        "        recommendations = get_recommendations(movie_id, final_df)\n",
        "\n",
        "        # Show recommendations\n",
        "        print(\"\\nTop 5 recommendations:\")\n",
        "        recommendations.show(truncate=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "    finally:\n",
        "        # Clean up\n",
        "        spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYbaJnjfy7LI"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col, udf, expr, broadcast, explode\n",
        "from pyspark.sql.types import ArrayType, StringType, FloatType, IntegerType\n",
        "import numpy as np\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"HybridMovieRecommender\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "def load_data():\n",
        "    # Load movies and ratings data\n",
        "    movies_df = spark.read.csv(\"\", header=True, inferSchema=True)\n",
        "    ratings_df = spark.read.csv(\"/content/rating.csv\", header=True, inferSchema=True)\n",
        "\n",
        "    # Ensure proper data types\n",
        "    movies_df = movies_df.withColumn(\"movieId\", col(\"movieId\").cast(IntegerType()))\n",
        "    ratings_df = ratings_df.withColumn(\"movieId\", col(\"movieId\").cast(IntegerType())) \\\n",
        "                          .withColumn(\"userId\", col(\"userId\").cast(IntegerType())) \\\n",
        "                          .withColumn(\"rating\", col(\"rating\").cast(FloatType()))\n",
        "\n",
        "    return movies_df, ratings_df\n",
        "\n",
        "def create_content_features(movies_df):\n",
        "    # Process movie titles\n",
        "    tokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"title_tokens\", pattern=\"\\\\W\")\n",
        "    remover = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"filtered_tokens\")\n",
        "    count_vectorizer = CountVectorizer(inputCol=\"filtered_tokens\", outputCol=\"title_tf\", minDF=2.0)\n",
        "    idf = IDF(inputCol=\"title_tf\", outputCol=\"title_tfidf\")\n",
        "\n",
        "    # Process genres\n",
        "    split_genres = udf(lambda x: x.split(\"|\") if x else [], ArrayType(StringType()))\n",
        "    movies_df = movies_df.withColumn(\"genres_array\", split_genres(col(\"genres\")))\n",
        "    genre_vectorizer = CountVectorizer(inputCol=\"genres_array\", outputCol=\"genre_features\", minDF=1.0)\n",
        "\n",
        "    # Create and apply pipeline\n",
        "    pipeline = Pipeline(stages=[\n",
        "        tokenizer, remover, count_vectorizer, idf, genre_vectorizer\n",
        "    ])\n",
        "\n",
        "    content_model = pipeline.fit(movies_df)\n",
        "    content_features_df = content_model.transform(movies_df)\n",
        "\n",
        "    # Combine features\n",
        "    assembler = VectorAssembler(\n",
        "        inputCols=[\"title_tfidf\", \"genre_features\"],\n",
        "        outputCol=\"combined_features\"\n",
        "    )\n",
        "    normalizer = Normalizer(inputCol=\"combined_features\", outputCol=\"normalized_features\")\n",
        "\n",
        "    feature_pipeline = Pipeline(stages=[assembler, normalizer])\n",
        "    final_features_df = feature_pipeline.fit(content_features_df).transform(content_features_df)\n",
        "\n",
        "    return final_features_df\n",
        "\n",
        "def train_collaborative_model(ratings_df):\n",
        "    # Train ALS model\n",
        "    als = ALS(maxIter=5,\n",
        "              regParam=0.01,\n",
        "              userCol=\"userId\",\n",
        "              itemCol=\"movieId\",\n",
        "              ratingCol=\"rating\",\n",
        "              coldStartStrategy=\"drop\",\n",
        "              nonnegative=True)\n",
        "\n",
        "    model = als.fit(ratings_df)\n",
        "    return model\n",
        "\n",
        "def get_content_similarity(movie_id, content_features_df, n=100):\n",
        "    # Get content-based similarities\n",
        "    input_movie = content_features_df.filter(col(\"movieId\") == movie_id).first()\n",
        "    input_vector = input_movie.normalized_features\n",
        "\n",
        "    def cosine_similarity(vec1, vec2):\n",
        "        return float(vec1.dot(vec2))\n",
        "\n",
        "    similarity_udf = udf(lambda x: cosine_similarity(input_vector, x), FloatType())\n",
        "\n",
        "    content_recommendations = content_features_df.filter(col(\"movieId\") != movie_id) \\\n",
        "        .withColumn(\"content_score\", similarity_udf(col(\"normalized_features\"))) \\\n",
        "        .select(\"movieId\", \"content_score\")\n",
        "\n",
        "    return content_recommendations\n",
        "\n",
        "def get_hybrid_recommendations(user_id, movie_id, content_features_df,\n",
        "                             collaborative_model, n=10, content_weight=0.3):\n",
        "\n",
        "    content_scores = get_content_similarity(movie_id, content_features_df)\n",
        "\n",
        "\n",
        "    users_df = spark.createDataFrame([(user_id,)], [\"userId\"])\n",
        "\n",
        "    # Generate recommendations using recommendForUserSubset\n",
        "    cf_recommendations = collaborative_model.recommendForUserSubset(users_df, 100)\n",
        "\n",
        "    # Explode the recommendations array and select required columns\n",
        "    cf_scores = cf_recommendations.select(\n",
        "        explode(\"recommendations\").alias(\"rec\")\n",
        "    ).select(\n",
        "        col(\"rec.movieId\").alias(\"movieId\"),\n",
        "        col(\"rec.rating\").alias(\"cf_score\")\n",
        "    )\n",
        "\n",
        "    # Join with content scores\n",
        "    hybrid_scores = cf_scores.join(broadcast(content_scores), \"movieId\", \"inner\")\n",
        "\n",
        "    # Calculate hybrid score and join with movie details\n",
        "    hybrid_recommendations = hybrid_scores \\\n",
        "        .withColumn(\"hybrid_score\",\n",
        "                   (1 - content_weight) * col(\"cf_score\") + content_weight * col(\"content_score\")) \\\n",
        "        .join(content_features_df.select(\"movieId\", \"title\", \"genres\"), \"movieId\") \\\n",
        "        .orderBy(col(\"hybrid_score\").desc()) \\\n",
        "        .select(\"movieId\", \"title\", \"genres\", \"hybrid_score\", \"cf_score\", \"content_score\") \\\n",
        "        .limit(n)\n",
        "\n",
        "    return hybrid_recommendations\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Load data\n",
        "        print(\"Loading data...\")\n",
        "        movies_df, ratings_df = load_data()\n",
        "\n",
        "        # Create content features\n",
        "        print(\"Creating content features...\")\n",
        "        content_features_df = create_content_features(movies_df)\n",
        "\n",
        "        # Train collaborative filtering model\n",
        "        print(\"Training collaborative filtering model...\")\n",
        "        collaborative_model = train_collaborative_model(ratings_df)\n",
        "\n",
        "        # Example usage\n",
        "        user_id = 1\n",
        "        movie_id = 1\n",
        "\n",
        "        print(f\"\\nGetting hybrid recommendations for user {user_id} based on movie {movie_id}...\")\n",
        "        recommendations = get_hybrid_recommendations(\n",
        "            user_id, movie_id, content_features_df, collaborative_model\n",
        "        )\n",
        "\n",
        "        # Show recommendations\n",
        "        print(\"\\nTop 10 Hybrid Recommendations:\")\n",
        "        recommendations.show(truncate=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "    finally:\n",
        "        spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}